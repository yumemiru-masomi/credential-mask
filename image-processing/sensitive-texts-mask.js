import vision from "@google-cloud/vision";
import { GoogleGenerativeAI } from "@google/generative-ai";
import { createCanvas, loadImage } from "@napi-rs/canvas";
import { writeFile } from "fs/promises";

async function quickstart() {
  // Creates clients
  const client = new vision.ImageAnnotatorClient();
  const genAI = new GoogleGenerativeAI(process.env.GOOGLE_GENERATIVE_AI);

  // Performs text detection on the image file
  const fileName = process.env.FILE_NAME;
  const [result] = await client.textDetection(fileName);
  const detections = result.textAnnotations;

  // Extract text into an array
  const textArray = detections
    .map((text, index) => {
      if (index === 0) {
        console.log(`Full Text: ${text.description}`);
      } else {
        return text.description;
      }
    })
    .filter(Boolean);

  console.log("Extracted Text Array:", textArray);

  // Prepare prompt for Generative AI
  const context = textArray.join(", ");
  const model = genAI.getGenerativeModel({
    model: "models/gemini-1.5-pro",
    generation_config: { response_mime_type: "application/json" },
  });

  const prompt = `ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®ä¸­ã‹ã‚‰ã€ãƒ–ãƒ­ã‚°å…¬é–‹æ™‚ã«éš ã—ãŸã»ã†ãŒè‰¯ã„æƒ…å ±ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚éš ã™ã¹ãæƒ…å ±ã®åŸºæº–ã¯æ¬¡ã®é€šã‚Šã§ã™ï¼š
1. APIã‚­ãƒ¼ï¼šã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã¨æ•°å­—ãŒæ··åœ¨ã™ã‚‹é•·ã„æ–‡å­—åˆ—ã€‚
2. ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼š'@' ã‚’å«ã‚€æ–‡å­—åˆ—ã€‚
3. é›»è©±ç•ªå·ï¼šæ•°å­—ã¨ '-' ãŒå«ã¾ã‚Œã‚‹å½¢å¼ï¼ˆä¾‹: "03-1234-5678"ï¼‰ã€‚
4. ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ç•ªå·ï¼š16æ¡ã®æ•°å­—ã€‚
5. å€‹äººåï¼šæ˜Žã‚‰ã‹ã«åå‰ã¨åˆ†ã‹ã‚‹ã‚‚ã®ã€‚
6. ä¼æ¥­åã‚„ã‚µãƒ¼ãƒ“ã‚¹åï¼šæ¬¡ã®ã‚ˆã†ãªç‰¹å¾´ã‚’æŒã¤ã‚‚ã®ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ï¼š
   - é€šå¸¸1ã¤ã¾ãŸã¯è¤‡æ•°ã®å˜èªžã§æ§‹æˆã•ã‚Œã€è¨˜å·ï¼ˆä¾‹: '-', '.'ï¼‰ã‚„ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆãŒæ··åœ¨ã™ã‚‹å ´åˆãŒå¤šã„ã€‚
   - æ–‡è„ˆã«åŸºã¥ã„ã¦ã€Œç‰¹å®šã®ãƒ–ãƒ©ãƒ³ãƒ‰ã€ã€Œä¼šç¤¾åã€ã€Œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã€ãªã©ã¨æŽ¨æ¸¬ã•ã‚Œã‚‹ã‚‚ã®ã€‚
   - ä¸€èˆ¬çš„ãªå˜èªžï¼ˆä¾‹: 'project', 'dashboard', 'add'ï¼‰ã¯å«ã¾ãªã„ã€‚
7. ä»¥ä¸‹ã®æ–‡å­—åˆ—ãŒå«ã¾ã‚Œã¦ã„ãŸã‚‰ã€ãã‚Œã¯éš ã™ã¹ãæƒ…å ±ã¨ã—ã¦ãã ã•ã„ã€‚
   - yumemi
   - yumemiru-masomi
   - Yumemi Saito


ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã€éš ã™ã¹ãæƒ…å ±ã‚’ JSON å½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚
å½¢å¼: { "sensitiveTexts": ["éš ã™ã¹ããƒ†ã‚­ã‚¹ãƒˆ1", "éš ã™ã¹ããƒ†ã‚­ã‚¹ãƒˆ2"] }
ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: ${context}`;

  console.log("Prompt for Generative AI:", prompt);
  console.log("ðŸŒŸ");
  // Call Google Generative AI SDK
  try {
    const resultAI = await model.generateContent(prompt);
    const resultAIResponse = resultAI.response.text();
    console.log("AI Response:", resultAIResponse);

    let cleanedResponse = resultAIResponse || "";
    console.log("cleanedResponse1", cleanedResponse);
    if (cleanedResponse.startsWith("```json")) {
      cleanedResponse = cleanedResponse
        .replace(/^```json\s*/, "") // å…ˆé ­ã® ```json ã¨æ”¹è¡Œã‚’å‰Šé™¤
        .replace(/\n```$/, "") // æœ«å°¾ã®æ”¹è¡Œã¨ ``` ã‚’å‰Šé™¤
        .trim(); // æ®‹ã£ãŸç©ºç™½ã‚’å‰Šé™¤
    }
    console.log("cleanedResponse2", cleanedResponse);
    if (cleanedResponse.endsWith("```")) {
      cleanedResponse = cleanedResponse
        .replace(/^```json\s*/, "") // å…ˆé ­ã® ```json ã¨æ”¹è¡Œã‚’å‰Šé™¤
        .replace(/```$/, "") // æœ«å°¾ã®æ”¹è¡Œã¨ ``` ã‚’å‰Šé™¤
        .trim(); // æ®‹ã£ãŸç©ºç™½ã‚’å‰Šé™¤
    }
    console.log("cleanedResponse3", cleanedResponse);

    // Parse AI response
    const sensitiveTexts = JSON.parse(cleanedResponse).sensitiveTexts;
    console.log("Sensitive Texts to Mask:", sensitiveTexts);

    // Save sensitive text array to a file
    await writeFile(
      "sensitiveTexts.json",
      JSON.stringify(sensitiveTexts, null, 2)
    );
    console.log("Sensitive text array saved as sensitiveTexts.json");

    // Load the image using @napi-rs/canvas
    const image = await loadImage(fileName);
    const imageWidth = image.width;
    const imageHeight = image.height;

    // Create a canvas to draw the image and bounding boxes
    const canvas = createCanvas(imageWidth, imageHeight);
    const ctx = canvas.getContext("2d");

    // Draw the image onto the canvas
    ctx.drawImage(image, 0, 0, imageWidth, imageHeight);

    // Draw red boxes for sensitive texts only
    detections.forEach((text, index) => {
      if (index !== 0 && sensitiveTexts.includes(text.description)) {
        console.log(`Text to mask: ${text.description}`);

        // Extract and log the bounding box coordinates
        const vertices = text.boundingPoly.vertices;
        const coordinates = vertices.map((v) => ({
          x: v.x || 0, // Default to 0 if x is undefined
          y: v.y || 0, // Default to 0 if y is undefined
        }));

        console.log("Bounding Box Coordinates:", coordinates);

        // Fill the bounding box completely
        ctx.fillStyle = "red"; // Set the fill color to solid red
        ctx.beginPath();
        ctx.moveTo(coordinates[0].x, coordinates[0].y);
        coordinates.forEach((vertex, i) => {
          if (i > 0) ctx.lineTo(vertex.x, vertex.y);
        });
        ctx.closePath();
        ctx.fill();

        // Draw the bounding box outline
        ctx.strokeStyle = "red"; // Set the color of the bounding box
        ctx.lineWidth = 2; // Set the line width
        ctx.beginPath();
        ctx.moveTo(coordinates[0].x, coordinates[0].y);
        coordinates.forEach((vertex, i) => {
          if (i > 0) ctx.lineTo(vertex.x, vertex.y);
        });
        ctx.closePath();
        ctx.stroke();
      } else {
        console.log(`Text not masked: ${text.description}`);
      }
    });

    // Save the annotated image to a file
    const buffer = canvas.toBuffer("image/png");
    await writeFile("output.png", buffer);
    console.log("Annotated image saved as output.png");
  } catch (error) {
    console.error("Error calling Generative AI API:", error);
  }
}

quickstart();
